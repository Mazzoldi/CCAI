{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea45971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langgraph langchain langchain_openai langchain_community psycopg2 fpdf nest_asyncio\n",
    "%pip install -q langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6caf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass, requests, psycopg2, datetime\n",
    "\n",
    "from typing import TypedDict, List, Optional, Annotated\n",
    "from IPython.display import Image, display\n",
    "from operator import add\n",
    "\n",
    "from langfuse import Langfuse\n",
    "from langfuse.callback import CallbackHandler\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.tools import tool\n",
    "import nest_asyncio\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c689461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "_set_env(\"SERPAPI_API_KEY\")\n",
    "_set_env(\"LANGFUSE_PUBLIC_KEY\")\n",
    "_set_env(\"LANGFUSE_SECRET_KEY\")\n",
    "_set_env(\"LANGFUSE_HOST\")\n",
    "_set_env(\"GOOGLE_API_KEY\")\n",
    "_set_env(\"GOOGLE_CSE_ID\")\n",
    "\n",
    "langfuse = Langfuse(\n",
    "    secret_key=os.environ[\"LANGFUSE_SECRET_KEY\"],\n",
    "    public_key=os.environ[\"LANGFUSE_PUBLIC_KEY\"],\n",
    "    host=os.environ[\"LANGFUSE_HOST\"]\n",
    ")\n",
    "\n",
    "langfuse_handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Draft(TypedDict):\n",
    "    title: str\n",
    "    body: str\n",
    "    image: str\n",
    "\n",
    "def update_draft(left: Draft | None, right: Draft | None) -> Draft:\n",
    "    if left is None:\n",
    "        return right\n",
    "    if right is None:\n",
    "        return left\n",
    "    draft = Draft(\n",
    "        title = right.get(\"title\", left.get(\"title\")),\n",
    "        body = right.get(\"body\", left.get(\"body\")),\n",
    "        image = right.get(\"image\", left.get(\"image\"))\n",
    "    )\n",
    "    return draft\n",
    "\n",
    "class Source(TypedDict):\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n",
    "    score: float\n",
    "\n",
    "class State(MessagesState):\n",
    "    prompt: str\n",
    "    topic: str\n",
    "    trending_topics: List[str]\n",
    "    exclude: Annotated[List[str], add]\n",
    "    fallback_to_trending: bool\n",
    "    sources: Optional[List[Source]]\n",
    "    draft: Annotated[Draft, update_draft]\n",
    "    feedback_title: Optional[str]\n",
    "    feedback_body: Optional[str]\n",
    "    feedback_image: Optional[str]\n",
    "    next: Optional[str]\n",
    "    article: Optional[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = os.getenv(\"POSTGRES_DB_NAME\")\n",
    "db_user = os.getenv(\"POSTGRES_USER\")\n",
    "db_psw = os.getenv(\"POSTGRES_PASSWORD\")\n",
    "db_host = os.getenv(\"POSTGRES_HOST\")\n",
    "db_port = os.getenv(\"POSTGRES_PORT\")\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname = db_name,\n",
    "    user = db_user,\n",
    "    password = db_psw,\n",
    "    host = db_host,\n",
    "    port = db_port\n",
    ")\n",
    "\n",
    "def save_article_to_db(article: str, topic: str):\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\n",
    "                \"INSERT INTO articles (topic, content) VALUES (%s, %s)\",\n",
    "                (topic.lower(), article)\n",
    "            )\n",
    "        conn.commit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving article to the database: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def get_topics_from_db() -> List[str]:\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT DISTINCT topic FROM articles\")\n",
    "            topics = [row[0] for row in cursor.fetchall()]\n",
    "        return topics\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching topics from the database: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0400d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def print_pdf(state: State) -> str:\n",
    "    \"\"\"\n",
    "    Generate a PDF from the article content and save it to the specified directory.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the generated PDF file.\n",
    "    \"\"\"\n",
    "    output_dir = \"./generated_articles\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    article = state[\"draft\"]\n",
    "    if not article:\n",
    "        raise ValueError(\"No article content provided.\")\n",
    "\n",
    "    title = article.get(\"title\", \"Untitled Article\")\n",
    "    body = article.get(\"body\", \"\")\n",
    "    image_path = article.get(\"image\", None)\n",
    "\n",
    "    filename = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_{title[:50].replace(' ', '_')}.pdf\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    pdf = FPDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "\n",
    "    pdf.set_font(\"Arial\", 'B', 16)\n",
    "    pdf.multi_cell(0, 10, title)\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        pdf.image(image_path, w=150)\n",
    "\n",
    "    pdf.set_font(\"Arial\", '', 12)\n",
    "    pdf.ln(10)\n",
    "    pdf.multi_cell(0, 10, body)\n",
    "\n",
    "    pdf.output(filepath)\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea224f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def print_markdown(state: State) -> str:\n",
    "    \"\"\"\n",
    "    Generate a Markdown file from the article content and save it to the specified directory.\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the generated Markdown file.\n",
    "    \"\"\"\n",
    "    output_dir = \"./generated_articles\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    article = state[\"article\"]\n",
    "    if not article:\n",
    "        raise ValueError(\"No article content provided.\")\n",
    "\n",
    "    filename = f\"article_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(article)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997af919",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    streaming=True,\n",
    "    verbose=True,\n",
    "    callbacks=[langfuse_handler],\n",
    ")\n",
    "\n",
    "tools = [print_pdf, print_markdown]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "tavily = TavilySearchResults(max_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e5a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_prompt_node(state: State) -> State:\n",
    "    '''\n",
    "    Get the user prompt node.\n",
    "    '''\n",
    "    prompt = input(\n",
    "        \"Inserisci il prompt per l'articolo: \"\n",
    "    )\n",
    "\n",
    "    return { \"prompt\": prompt }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8542f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_node(state: State) -> State:\n",
    "    '''\n",
    "    Decide whether to extract a topic or call the tool to get trending topics.\n",
    "    If the user has clearly provided a topic, extract it.\n",
    "    Otherwise, trigger the get_trending_topics tool.\n",
    "    '''\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": '''\n",
    "                You are a helpful assistant that decides if a user has provided a clear article topic or needs suggestions.\n",
    "\n",
    "                - If the user prompt clearly contains a topic, extract it as a short phrase.\n",
    "                  For example, \"How to train a dog\" -> \"dog training\" or \"The history of the internet\" -> \"internet history\".\n",
    "                \n",
    "                - If the user is asking for suggestions or is too vague, respond with a function call to `get_trending_topics`.\n",
    "                  For example, \"What should I write about?\" or \"I need ideas for an article\" should trigger the function call.\n",
    "                \n",
    "                Respond ONLY in one of the following formats:\n",
    "                - To extract a topic: \"topic: <topic>\"\n",
    "                - To call the function: \"function: get_trending_topics\"\n",
    "\n",
    "                If the user gives a topic, extract in in english and remove any special characters or punctuation.\n",
    "                '''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": state[\"prompt\"]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    content = response.content\n",
    "\n",
    "    if content.startswith(\"topic:\"):\n",
    "        topic = content.split(\"topic:\")[1].strip()\n",
    "        return { \"topic\": topic }\n",
    "    elif content.startswith(\"function:\"):\n",
    "        return { \"topic\": None }\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response format: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63480d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_fallback_flag_node(state: State) -> State:\n",
    "    '''\n",
    "    Set the fallback flag to True if the source research does not return enough results.\n",
    "    '''\n",
    "    state[\"fallback_to_trending\"] = True\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ba486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_old_topics_node(state: State) -> State:\n",
    "    '''\n",
    "    Get the old topics from the database.\n",
    "    '''\n",
    "    \n",
    "    return { \"exclude\": get_topics_from_db() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb091689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trending_topics_node(state: State) -> State:\n",
    "    '''\n",
    "    Get three the trending topics from Google Trends.\n",
    "    Returns them and asks the user to choose one.\n",
    "    Exclude the topics already proposed to the user.\n",
    "    '''\n",
    "\n",
    "    if state.get(\"fallback_to_trending\", False):\n",
    "        print(\"\\nNon sono state trovate fonti sufficienti per il topic scelto.\\nTi propongo tre nuovi argomenti di tendenza.\\n\", flush=True)\n",
    "\n",
    "    exclude = state.get(\"exclude\", None)\n",
    "\n",
    "    url = \"https://serpapi.com/search\"\n",
    "    params = {\n",
    "        \"engine\": \"google_trends_trending_now\",\n",
    "        \"api_key\": os.environ[\"SERPAPI_API_KEY\"],\n",
    "        \"geo\": \"IT\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Error fetching trending topics: {response.status_code}\")\n",
    "    data = response.json()\n",
    "\n",
    "    trending_searches = []\n",
    "    for trend in data.get(\"trending_searches\", []):\n",
    "        query = trend.get(\"query\")\n",
    "        if query:\n",
    "            trending_searches.append(query)    \n",
    "\n",
    "    if exclude is not None:\n",
    "        trending_searches = [t for t in trending_searches if t not in exclude]\n",
    "\n",
    "    return { \"trending_topics\": trending_searches[:3] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a918b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_or_choose_topic_node(state: State) -> State:\n",
    "    '''\n",
    "    Suggest the trending topics to the user and ask him to choose one.\n",
    "    Extract the topic from the user input if he chooses one, otherwise call the function again.\n",
    "    '''\n",
    "\n",
    "    trending_topics = state.get(\"trending_topics\")\n",
    "\n",
    "    suggestions = \"\\n\".join([f\"{i+1}. {t}\" for i, t in enumerate(trending_topics)])\n",
    "    print(f\"\\nEcco tre topic di tendenza:\\n{suggestions}\\n\", flush=True)\n",
    "\n",
    "    user_reply = input(\"Scrivi il topic che preferisci (anche non presente nella lista) o chiedi di vederne altri: \")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": '''\n",
    "                You are a helpful assistant who suggests trending topics to the user and helps him choose one.\n",
    "\n",
    "                You have suggested the following topics:\n",
    "                {suggestions}\n",
    "\n",
    "                Interpret the user reply:\n",
    "                - If the user chooses one of the suggested topics, extract it.\n",
    "                - If the user suggests a different topic, extract that.\n",
    "                - If the user asks for more suggestions, respond exactly with \"more_topics\".\n",
    "\n",
    "                Respond ONLY in one of the following formats:\n",
    "                - To extract a topic: \"topic: <topic>\"\n",
    "                - To get more topics: \"more_topics\"\n",
    "\n",
    "                If the user gives a topic, extract it in english and remove any special characters or punctuation.\n",
    "                '''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_reply\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    content = response.content.strip()\n",
    "\n",
    "    if content.startswith(\"topic:\"):\n",
    "        topic = content.split(\"topic:\")[1].strip()\n",
    "        return { \"topic\": topic }\n",
    "    elif content == \"more_topics\":\n",
    "        return { \n",
    "            \"topic\": None,\n",
    "            \"exclude\": trending_topics,\n",
    "            }\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response format: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sources_node(state: State) -> State:\n",
    "    '''\n",
    "    Use Tavily to search for high-quality sources based on the topic.\n",
    "    Filters results with score > 0.7 and keeps structured data.\n",
    "    '''\n",
    "    \n",
    "    results = tavily.invoke({ \"query\": state[\"topic\"] })\n",
    "\n",
    "    formatted_results: List[Source] = [\n",
    "        Source(\n",
    "            title=r.get(\"title\", \"\"),\n",
    "            url=r.get(\"url\", \"\"),\n",
    "            content=r.get(\"content\", \"\"),\n",
    "            score=r.get(\"score\", 0),\n",
    "        )\n",
    "        for r in results if r.get(\"score\", 0) > 0.7\n",
    "    ]\n",
    "\n",
    "    return { \"sources\": formatted_results }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb326768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_router(state: State) -> State:\n",
    "    '''\n",
    "    Route to the next state based on the current draft state.\n",
    "    '''\n",
    "    \n",
    "    if state.get(\"draft\", {}).get(\"title\") is None or state.get(\"feedback_title\") is not None:\n",
    "        return { \"next\": \"generate_title\" }\n",
    "\n",
    "    if state.get(\"draft\", {}).get(\"body\") is None or state.get(\"feedback_body\") is not None:\n",
    "        return { \"next\": \"generate_body\" }\n",
    "\n",
    "    if not state.get(\"draft\", {}).get(\"image\") or state.get(\"feedback_image\") is not None:\n",
    "        return { \"next\": \"generate_image\" }\n",
    "\n",
    "    return { \"next\": \"render_article\" }  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6d0ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_title_node(state: State) -> State:\n",
    "    '''\n",
    "    Generate a title for the article based on the topic and sources.\n",
    "    '''\n",
    "\n",
    "    prompt = f'''\n",
    "    You are an expert article writer.\n",
    "    Your task is to generate a catchy and clickbait title for the article based on the following topic and sources.\n",
    "\n",
    "    Topic: {state[\"topic\"]}\n",
    "    Sources: {state[\"sources\"]}\n",
    "    '''\n",
    "\n",
    "    if state.get(\"feedback_title\") is not None:\n",
    "        prompt += f'''\\n\n",
    "        You have received the following feedback on the title: \n",
    "        {state[\"feedback_title\"]}.\n",
    "        Use this feedback to improve the title.\n",
    "        For example, if the feedback is \"too long\", make the title shorter.\n",
    "        If the feedback is \"not engaging\", make the title more engaging.\n",
    "        '''\n",
    "\n",
    "    prompt += '''\\n\n",
    "    The title should be engaging and reflect the content of the article.\n",
    "    Exagerate the topic and make it sound interesting.\n",
    "    The title should be no more than 10 words long and should not contain any special characters or numbers.\n",
    "    For example, if the topic is \"dog training\" and the sources are about dog training techniques, a possible title could be \"The ultimate guide to dog training: tips and tricks\".\n",
    "    Only return the title.\n",
    "    Do not include any additional text or explanations.\n",
    "    The language of the title should be ITALIAN.\n",
    "    '''\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    title = response.content.strip()\n",
    "    return {\n",
    "        \"draft\": {\n",
    "            \"title\": title\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_body_node(state: State) -> State:\n",
    "    '''\n",
    "    Generate a draft for the article based on the topic and sources.\n",
    "    '''\n",
    "    \n",
    "    urls = [source[\"url\"] for source in state[\"sources\"] if source[\"url\"]]\n",
    "    sources = \"\\n\".join(urls)\n",
    "    \n",
    "    prompt = f'''\n",
    "    You are a professional article writer.\n",
    "    You have to write a detailed draft about the following topic: \"{state[\"topic\"]}\".    \n",
    "\n",
    "    Use **ONLY** the exact information contained in the following sources, and do not add anything that is not directly supported by them. \n",
    "    The draft should strictly reflect the facts as they are presented in the sources without any additional assumptions or unverified details:\n",
    "    {sources}\n",
    "    '''\n",
    "\n",
    "    if state.get(\"feedback_body\") is not None:\n",
    "        prompt += f'''\\n\n",
    "        You have received the following feedback by the user: \n",
    "        \"{state[\"feedback_body\"]}\".\n",
    "        Use this feedback to improve the draft.\n",
    "        '''\n",
    "\n",
    "    prompt += f'''\\n\n",
    "    Make sure to mention the exact dates and details from the sources, without adding any speculative or external information. \n",
    "    The tone should be formal, informative, and objective.\n",
    "    The article should be at least 500 words long and should not contain any grammatical or spelling errors.\n",
    "    The language of the article should be ITALIAN.\n",
    "    '''\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    body = response.content.strip()\n",
    "    return {\n",
    "        \"draft\": {\n",
    "            \"body\": body\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb073ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_node(state: State) -> State:\n",
    "    '''\n",
    "    Generate a query for Google Custom Search Engine to find images related to the topic.\n",
    "    Display the first image and update the draft with the image URL.\n",
    "    '''\n",
    "\n",
    "    prompt = f'''\n",
    "    You are an image search agent.\n",
    "    Your task is to generate a query to pass to the Google API to search for images related to the following topic: \"{state[\"topic\"]}\".\n",
    "    Return only the query, without any additional text.\n",
    "    \n",
    "    For example, if the topic is \"World Cup final 2022\", the query could be \"world cup final 2022\".\n",
    "    '''\n",
    "\n",
    "    if state.get(\"feedback_image\") is not None:\n",
    "        prompt += f'''\\n\n",
    "        You have received the following feedback by the user: \n",
    "        \"{state[\"feedback_image\"]}\".\n",
    "        Use this feedback to improve the query.\n",
    "        For example, if the feedback is \"I want images of Messi\", make sure to include \"Messi\" in the query.\n",
    "        If the topic is \"World Cup final 2022\" and the feedback is \"I want images of Messi\", the query could be \"Messi world cup final 2022\".\n",
    "        If the topic is \"Presidential elections USA 2024\" and the feedback is \"I want images of Trump\", the query could be \"Trump presidential elections USA 2024\".\n",
    "        '''\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    query = response.content.strip()\n",
    "\n",
    "    api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "    cx = os.getenv(\"GOOGLE_CSE_ID\")\n",
    "    if not api_key or not cx:\n",
    "        raise RuntimeError(\"Setta GOOGLE_API_KEY e GOOGLE_CSE_ID nell'ambiente\")\n",
    "    \n",
    "    endpoint = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        \"key\": api_key,\n",
    "        \"cx\": cx,\n",
    "        \"q\": query,\n",
    "        \"searchType\": \"image\",\n",
    "        \"num\": 1,\n",
    "        \"imgSize\": \"medium\",\n",
    "        \"imgColorType\": \"color\",\n",
    "    }\n",
    "\n",
    "    resp = requests.get(endpoint, params=params)\n",
    "    resp.raise_for_status()\n",
    "    items = resp.json().get(\"items\", [])\n",
    "\n",
    "    url: str = items[0][\"link\"] if items else None\n",
    "\n",
    "    return {\n",
    "        \"draft\": {\n",
    "            \"image\": url\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af207d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_feedback_node(state: State) -> State:\n",
    "    '''\n",
    "    Ask the user for feedback on the generated title of the article.\n",
    "    The system will interpret the feedback naturally.\n",
    "    '''\n",
    "\n",
    "    print(f\"\\nEcco il titolo dell'articolo:\\n{state['draft']['title']}\\n\", flush=True)\n",
    "    feedback = input(\"C'è qualcosa che vorresti migliorare o cambiare? \")\n",
    "\n",
    "    prompt = f'''\n",
    "    You are a helpful assistant who interprets user feedback on a title for an article.\n",
    "    The user has provided the following feedback: \"{feedback}\".\n",
    "\n",
    "    The title of the article is: \"{state[\"draft\"][\"title\"]}\".\n",
    "    \n",
    "    Interpret the feedback and decide if the user is satisfied with the title or if he wants to change it.\n",
    "    - If the user is satisfied, respond with \"no_changes\".\n",
    "    - If the user wants to change the title, extract the changes he wants to make and respond with \"changes: <changes>\".\n",
    "      For example, if the user says \"I don't want the title to be title case\", respond with \"changes: remove title case\".\n",
    "\n",
    "    Do not include any additional text or explanations.\n",
    "    '''\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    content = response.content.strip()\n",
    "\n",
    "    if content == \"no_changes\":\n",
    "        return { \"feedback_title\": None }\n",
    "    elif content.startswith(\"changes:\"):\n",
    "        changes = content.split(\"changes:\")[1].strip()\n",
    "        return { \"feedback_title\": changes }\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response format: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body_feedback_node(state: State) -> State:\n",
    "    '''\n",
    "    Ask the user for feedback on the generated body of the article.\n",
    "    The system will interpret the feedback naturally.\n",
    "    '''\n",
    "    \n",
    "    print(f\"\\nEcco il corpo dell'articolo:\\n{state['draft']['body']}\\n\", flush=True)\n",
    "    feedback = input(\"C'è qualcosa che vorresti migliorare o cambiare? \")\n",
    "\n",
    "    prompt = f'''\n",
    "    You are a helpful assistant that interprets user feedback on an article draft.\n",
    "    The user has provided the following feedback: \"{feedback}\".\n",
    "\n",
    "    Based on this feedback, decide if the user wants to change something in the article body.\n",
    "    Use ONLY the following format to respond:\n",
    "    - If the user wants to change something, extract the specific changes they want to make and respond with \"changes: <changes>\".\n",
    "      For example, if the user says \"Make it more detailed\", respond with \"changes: add more details\".\n",
    "      If the user says \"Make it shorter\", respond with \"changes: shorten the article\".\n",
    "      If the user says \"Add more examples\", respond with \"changes: add more examples\".\n",
    "    - If the user is satisfied with the draft, respond with \"no_changes\".\n",
    "    \n",
    "    Do not include any additional text or explanations.\n",
    "    Respond ONLY in one of the following formats:\n",
    "    - To extract changes: \"changes: <changes>\"\n",
    "    - To indicate no changes: \"no_changes\"\n",
    "    '''\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    content = response.content.strip()\n",
    "\n",
    "    if content.startswith(\"changes:\"):\n",
    "        changes = content.split(\"changes:\")[1].strip()\n",
    "        return { \"feedback_body\": changes }\n",
    "    elif content == \"no_changes\":\n",
    "        return { \"feedback_body\": None }\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response format: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a6db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feedback_node(state: State) -> State:\n",
    "    '''\n",
    "    Ask the user for feedback on the generated image of the article.\n",
    "    The system will interpret the feedback naturally.\n",
    "    '''\n",
    "\n",
    "    print(f\"\\nEcco l'immagine dell'articolo:\\n\", flush=True)\n",
    "    display(Image(url=state[\"draft\"][\"image\"]))\n",
    "\n",
    "    feedback = input(\"C'è qualcosa che vorresti migliorare o cambiare? \")\n",
    "\n",
    "    prompt = f'''\n",
    "    You are a helpful assistant that interprets user feedback on an article image.\n",
    "    The user has provided the following feedback: \"{feedback}\".\n",
    "\n",
    "    Based on this feedback, decide if the user wants to change something in the image.\n",
    "    Use ONLY the following format to respond:\n",
    "    - If the user wants to change something, extract the specific changes they want to make and respond with \"changes: <changes>\".\n",
    "      For example, if the user says \"The image must include a dog\", respond with \"changes: include a dog\".\n",
    "      If the user says \"Add a logo\", respond with \"changes: add a logo\".\n",
    "    - If the user is satisfied with the image, respond with \"no_changes\".\n",
    "    \n",
    "    Do not include any additional text or explanations.\n",
    "    Respond ONLY in one of the following formats:\n",
    "    - To extract changes: \"changes: <changes>\"\n",
    "    - To indicate no changes: \"no_changes\"\n",
    "    '''\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    content = response.content.strip()\n",
    "\n",
    "    if content.startswith(\"changes:\"):\n",
    "        changes = content.split(\"changes:\")[1].strip()\n",
    "        return { \"feedback_image\": changes }\n",
    "    elif content == \"no_changes\":\n",
    "        return { \"feedback_image\": None }\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected response format: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb073ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_article_node(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Render the final article in Markdown format, including the title, body, and image.\n",
    "    Saves the article into the database.\n",
    "    \"\"\"\n",
    "\n",
    "    title = state.get(\"draft\", {}).get(\"title\", \"\")\n",
    "    body = state.get(\"draft\", {}).get(\"body\", \"\")\n",
    "    image = state.get(\"draft\", {}).get(\"image\", \"\")\n",
    "\n",
    "    prompt = f'''\n",
    "    You are a professional article writer.\n",
    "    Your task is to render the final article in Markdown format, including the title, body, and image.\n",
    "\n",
    "    The article should be well-structured and easy to read.\n",
    "    The title should be in H1 format, the body should be in paragraphs.\n",
    "    The image should be put below the title and above the body.\n",
    "\n",
    "    If the body contains a title itself, remove it and keep only the content.\n",
    "\n",
    "    Here are the details:\n",
    "    Title: {title}\n",
    "    Body: {body}\n",
    "    Image: {image}\n",
    "\n",
    "    The image is a url link to the image.\n",
    "    So, it should be rendered as a Markdown image link, like this:\n",
    "    ![Image](<image_url>)\n",
    "\n",
    "    Do not include any additional text or explanations.\n",
    "    Only return the final Markdown text.\n",
    "    The language of the article should be ITALIAN.\n",
    "    '''\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    article = response.content.strip()\n",
    "\n",
    "    save_article_to_db(article, state[\"topic\"])\n",
    "\n",
    "    return { \"article\": article }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cdb724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_post_article_node(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Handle the post-article state.\n",
    "    Ask the user if he wants to post the article on social media or print it.\n",
    "    The only options are to post it on X (Twitter) or print it as a PDF or Markdown file.\n",
    "    \"\"\"\n",
    "\n",
    "    feedback = input(\"Vuoi stampare l'articolo? \")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": '''\n",
    "                You are a helpful assistant that interprets user feedback on an article post.\n",
    "                The user has generated an article and is now deciding whether to print it or not.\n",
    "                The user has the following options:\n",
    "                - Print it as a PDF\n",
    "                - Print it as a Markdown file\n",
    "                - Do not print it AT ALL\n",
    "\n",
    "                The user was asked if he wanted to print the article and he replied: \"{feedback}\".\n",
    "\n",
    "                Based on this feedback, decide if the user wants to print the article or not.\n",
    "                You can call the following tools:\n",
    "                - print_pdf: to print the article as a PDF\n",
    "                - print_markdown: to print the article as a Markdown file\n",
    "                - __end__: to indicate that the user does NOT want to print the article\n",
    "\n",
    "                Decide which tool to call based on the user's feedback.\n",
    "                \n",
    "                Do not include any additional text or explanations.\n",
    "            '''\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": feedback\n",
    "        }\n",
    "    ]\n",
    "    response = llm_with_tools.invoke(messages + state.get(\"messages\", []))\n",
    "    print(response)\n",
    "    if \"__end__\" in response:\n",
    "        print(\"L'utente non vuole stampare l'articolo.\")\n",
    "    elif \"print_pdf\" in response:\n",
    "        print(\"L'utente ha scelto di stampare l'articolo come PDF.\")\n",
    "    elif \"print_markdown\" in response:\n",
    "        print(\"L'utente ha scelto di stampare l'articolo come Markdown.\")\n",
    "    return { \"messages\": state.get(\"messages\", []) + [response] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c12db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(State)\n",
    "\n",
    "builder.add_node(\"get_user_prompt\", get_user_prompt_node)\n",
    "builder.add_node(\"router\", router_node)\n",
    "builder.add_node(\"get_old_topics\", get_old_topics_node)\n",
    "builder.add_node(\"get_trending_topics\", get_trending_topics_node)\n",
    "builder.add_node(\"set_fallback_flag\", set_fallback_flag_node)\n",
    "builder.add_node(\"suggest_or_choose_topic\", suggest_or_choose_topic_node)\n",
    "builder.add_node(\"get_sources\", get_sources_node)\n",
    "builder.add_node(\"draft_router\", draft_router)\n",
    "builder.add_node(\"generate_body\", generate_body_node)\n",
    "builder.add_node(\"generate_title\", generate_title_node)\n",
    "builder.add_node(\"generate_image\", generate_image_node)\n",
    "builder.add_node(\"title_feedback\", get_title_feedback_node)\n",
    "builder.add_node(\"body_feedback\", get_body_feedback_node)\n",
    "builder.add_node(\"image_feedback\", get_image_feedback_node)\n",
    "builder.add_node(\"render_article\", render_article_node)\n",
    "builder.add_node(\"handle_post_article\", handle_post_article_node)\n",
    "builder.add_node(\"tools\", ToolNode([print_pdf, print_markdown]))\n",
    "\n",
    "builder.add_edge(START, \"get_user_prompt\")\n",
    "builder.add_edge(\"get_user_prompt\", \"router\")\n",
    "builder.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda state: \"topic\" if state[\"topic\"] else \"no_topic\",\n",
    "    path_map = {\n",
    "        \"topic\": \"get_sources\",\n",
    "        \"no_topic\": \"get_old_topics\"\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"get_old_topics\", \"get_trending_topics\")\n",
    "builder.add_edge(\"get_trending_topics\", \"suggest_or_choose_topic\")\n",
    "builder.add_conditional_edges(\n",
    "    \"suggest_or_choose_topic\",\n",
    "    lambda state: \"chosen\" if state[\"topic\"] else \"more\",\n",
    "    path_map = {\n",
    "        \"chosen\": \"get_sources\",\n",
    "        \"more\": \"get_trending_topics\"\n",
    "    }\n",
    ")\n",
    "builder.add_conditional_edges(\n",
    "    \"get_sources\",\n",
    "    lambda state: \"enough\" if state[\"sources\"] and len(state[\"sources\"]) >= 2 else \"not_enough\",\n",
    "    path_map = {\n",
    "        \"enough\": \"draft_router\",\n",
    "        \"not_enough\": \"set_fallback_flag\"\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"set_fallback_flag\", \"get_trending_topics\")\n",
    "builder.add_conditional_edges(\n",
    "    \"draft_router\",\n",
    "    lambda output: output[\"next\"],\n",
    "    path_map={\n",
    "        \"generate_title\": \"generate_title\",\n",
    "        \"generate_body\": \"generate_body\",\n",
    "        \"generate_image\": \"generate_image\",\n",
    "        \"render_article\": \"render_article\",\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"generate_title\", \"title_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"title_feedback\",\n",
    "    lambda state: \"approved\" if state.get(\"feedback_title\") is None else \"rejected\",\n",
    "    path_map={\n",
    "        \"approved\": \"draft_router\",\n",
    "        \"rejected\": \"generate_title\"\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"generate_body\", \"body_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"body_feedback\",\n",
    "    lambda state: \"approved\" if state.get(\"feedback_body\") is None else \"rejected\",\n",
    "    path_map={\n",
    "        \"approved\": \"draft_router\",\n",
    "        \"rejected\": \"generate_body\"\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"generate_image\", \"image_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"image_feedback\",\n",
    "    lambda state: \"approved\" if state.get(\"feedback_image\") is None else \"rejected\",\n",
    "    path_map={\n",
    "        \"approved\": \"draft_router\",\n",
    "        \"rejected\": \"generate_image\"\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"render_article\", \"handle_post_article\")\n",
    "builder.add_conditional_edges(\n",
    "    \"handle_post_article\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"handle_post_article\")\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph = builder.compile().with_config(\n",
    "    callbacks=[langfuse_handler],\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670bf27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    graph.invoke({})\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ecc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    nest_asyncio.apply()\n",
    "    display(Image(graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.PYPPETEER)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
